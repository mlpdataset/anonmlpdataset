{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary wrapper\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "annotations = {\n",
    "  'mrw':  ['mrw-v1.0.json'],\n",
    "  'tgif': ['tgif-v1.0.tsv'],\n",
    "  'coco': ['annotations/captions_train2014.json',\n",
    "           'annotations/captions_val2014.json'],\n",
    "  'lp': ['ml-1.json']\n",
    "}\n",
    "\n",
    "class Vocabulary(object):\n",
    "  \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.idx = 0\n",
    "    self.word2idx = {}\n",
    "    self.idx2word = {}\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word2idx:\n",
    "      self.word2idx[word] = self.idx\n",
    "      self.idx2word[self.idx] = word\n",
    "      self.idx += 1\n",
    "\n",
    "  def __call__(self, word):\n",
    "    if word not in self.word2idx:\n",
    "      return self.word2idx['<unk>']\n",
    "    return self.word2idx[word]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.word2idx)\n",
    "\n",
    "\n",
    "def from_tgif_tsv(path):\n",
    "  captions = [line.strip().split('\\t')[1] \\\n",
    "       for line in open(path, 'r').readlines()]\n",
    "  return captions\n",
    "\n",
    "\n",
    "def from_mrw_json(path):\n",
    "  dataset = json.load(open(path, 'r'))\n",
    "  captions = []\n",
    "  for i, datum in enumerate(dataset):\n",
    "    cap = datum['sentence']\n",
    "    cap = cap.replace('/r/','')\n",
    "    cap = cap.replace('r/','')\n",
    "    cap = cap.replace('/u/','')\n",
    "    cap = cap.replace('u/','')\n",
    "    cap = cap.replace('..','')\n",
    "    cap = cap.replace('/',' ')\n",
    "    cap = cap.replace('-',' ')\n",
    "    captions += [cap]\n",
    "  return captions\n",
    "\n",
    "\n",
    "def from_coco_json(path):\n",
    "  coco = COCO(path)\n",
    "  ids = coco.anns.keys()\n",
    "  captions = []\n",
    "  for i, idx in enumerate(ids):\n",
    "    captions.append(str(coco.anns[idx]['caption']))\n",
    "\n",
    "  return captions\n",
    "\n",
    "def from_lp_json(path):\n",
    "  dataset = json.load(open(path, 'r'))\n",
    "  captions = []\n",
    "  for i, datum in enumerate(dataset):\n",
    "    \n",
    "    cap = [i['Word'] for i in dataset[datum]['captions']] + [i['text'] for i in dataset[datum]['slide_text']]\n",
    "    \n",
    "    try:\n",
    "        captions += [\" \".join(cap)]\n",
    "    except Exception:\n",
    "        pdb.set_trace()\n",
    "  return captions\n",
    "\n",
    "\n",
    "def from_txt(txt):\n",
    "  captions = []\n",
    "  with open(txt, 'rb') as f:\n",
    "    for line in f:\n",
    "      captions.append(line.strip())\n",
    "  return captions\n",
    "\n",
    "\n",
    "def build_vocab(data_name = 'lp', jsons = ['./ml-1.json'], threshold = 0):\n",
    "  \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "  import nltk\n",
    "  counter = Counter()\n",
    "  for path in jsons:\n",
    "    full_path = path\n",
    "    if data_name == 'tgif':\n",
    "      captions = from_tgif_tsv(full_path)\n",
    "    elif data_name == 'mrw':\n",
    "      captions = from_mrw_json(full_path)\n",
    "    elif data_name == 'coco':\n",
    "      captions = from_coco_json(full_path)\n",
    "    elif data_name == 'lp':\n",
    "      captions = from_lp_json(full_path)\n",
    "    else:\n",
    "      captions = from_txt(full_path)\n",
    "\n",
    "    for i, caption in enumerate(captions):\n",
    "      tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "      counter.update(tokens)\n",
    "\n",
    "  # Discard if the occurrence of the word is less than min_word_cnt.\n",
    "  words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "  print('Vocabulary size: {}'.format(len(words)))\n",
    "\n",
    "  # Create a vocab wrapper and add some special tokens.\n",
    "  vocab = Vocabulary()\n",
    "  vocab.add_word('<pad>')\n",
    "  vocab.add_word('<start>')\n",
    "  vocab.add_word('<end>')\n",
    "  vocab.add_word('<unk>')\n",
    "\n",
    "  # Add words to the vocabulary.\n",
    "  for i, word in enumerate(words):\n",
    "    vocab.add_word(word)\n",
    "  return vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9087\n",
      "Saved vocabulary file to  /bio-1_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "speakers = [\"ml-1\", \"anat-1\", \"bio-1\", \"bio-3\", \"dental\", \"psy-2\", \"anat-2\",  \"bio-2\", \"bio-4\", \"psy-1\", \"speaking\"]\n",
    "import os \n",
    "speakers = [\"bio-1\"]\n",
    "\n",
    "for sp in speakers:\n",
    "    jsons = [os.path.join(\"/projects/dataset_processed/dongwonl/data/{}/{}.json\".format(sp,sp))]\n",
    "    vocab = build_vocab(data_name = 'lp', jsons = jsons)  \n",
    "    with open('./%s_vocab.pkl' % sp, 'wb') as f:\n",
    "      pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Saved vocabulary file to \", '/%s_vocab.pkl' % sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
